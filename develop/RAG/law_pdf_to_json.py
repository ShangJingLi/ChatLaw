import os
import re
import json
from pathlib import Path
import fitz  # PyMuPDF
from transformers import AutoTokenizer, AutoModelForCausalLM

# ================= é…ç½® =================
PDF_DIR = "/home/stu1/li/ChatLaw/develop/RAG/output/Legal Documents"
OUTPUT_DIR = "./output_json"

CURRENT_FILE = Path(__file__).resolve()
PROJECT_ROOT = CURRENT_FILE.parents[2]

MODEL_PATH = PROJECT_ROOT / "chatlaw" / "resources" / "llm"
TOKENIZER_PATH = PROJECT_ROOT / "chatlaw" / "resources" / "tokenizer"

MAX_INPUT_CHARS = 2000

# ================= å·¥å…·å‡½æ•° =================
def extract_pdf_text(pdf_path: Path) -> str:
    doc = fitz.open(pdf_path)
    pages = []

    for page in doc:
        text = page.get_text()
        # è¡Œå†…æ–­è¡Œ â†’ ç©ºæ ¼
        text = re.sub(r"(?<!\n)\n(?!\n)", " ", text)
        # å¤šç©ºè¡Œå‹ç¼©
        text = re.sub(r"\n{2,}", "\n", text)
        pages.append(text)

    doc.close()
    return "\n".join(pages)


def normalize_article_starts(text: str) -> str:
    """
    è®©çœŸæ­£çš„â€œç¬¬Xæ¡ â€¦â€¦â€å°½é‡å‡ºç°åœ¨æ–°è¡Œå¼€å¤´ï¼Œæ¢å¤ç»“æ„é”šç‚¹ã€‚
    æ³¨æ„ï¼šä¸ä¼šæ¸…æ´—å†…å®¹ï¼Œåªæ˜¯æ’å…¥æ¢è¡Œç”¨äºåˆ†æ®µã€‚
    """
    # åœ¨â€œç¬¬Xæ¡â€å‰æ’å…¥æ¢è¡Œï¼Œä½†é¿å…æŠŠâ€œâ€¦â€¦ç¬¬åäºŒæ¡è§„å®š/è‡³â€¦â€¦â€è¿™ç§å¥ä¸­å¼•ç”¨å½“æˆæ–°æ¡
    text = re.sub(
        r"(?<!\n)(?<![ä¸€-é¾¥])"                 # å‰é¢ä¸æ˜¯æ¢è¡Œã€ä¹Ÿä¸æ˜¯æ±‰å­—ï¼ˆå°½é‡é¿å…å¥ä¸­å¼•ç”¨ï¼‰
        r"(ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+\s*æ¡)"
        r"(?!\s*(è‡³|çš„|è§„å®š))",                # åé¢ä¸æ˜¯â€œè‡³/çš„/è§„å®šâ€
        r"\n\1",
        text
    )
    return text


def load_model():
    tokenizer = AutoTokenizer.from_pretrained(
        TOKENIZER_PATH,
        local_files_only=True
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        local_files_only=True,
        device_map="auto",
        dtype="auto",
        attn_implementation="sdpa"
    )

    return model, tokenizer


# ================= LLMï¼šæ¸…æ´— =================

def llm_clean_article(model, tokenizer, raw_text: str) -> str:
    messages = [
        {
            "role": "user",
            "content": f"""ä¸‹é¢æ˜¯ä¸€æ¡æ³•å¾‹æ¡æ–‡çš„åŸå§‹æ–‡æœ¬ï¼Œå¯èƒ½åŒ…å«æ–­è¡Œã€å¤šä½™ç©ºæ ¼ã€é¡µç ç­‰å™ªå£°ã€‚
                è¯·åœ¨ã€ä¸æ”¹å˜æ³•å¾‹å«ä¹‰ã€ä¸æ–°å¢å†…å®¹ã€‘çš„å‰æä¸‹ï¼š
                1. åˆå¹¶æ–­è¡Œ
                2. åˆ é™¤é¡µç ï¼ˆå¦‚â€œï¼2ï¼â€ï¼‰
                3. åˆ é™¤æ˜æ˜¾å¤šä½™çš„ç©ºæ ¼
                4. è¾“å‡ºä¸€æ¡å¹²å‡€ã€è¿ç»­ã€é€‚åˆå­˜å…¥ JSON çš„æ¡æ–‡æ–‡æœ¬
                
                åªè¾“å‡ºæ¸…æ´—åçš„æ¡æ–‡æ­£æ–‡ï¼Œä¸è¦è§£é‡Šã€‚
                
                åŸæ–‡å¦‚ä¸‹ï¼š
                {raw_text}
                """
        }
    ]

    chat_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )

    inputs = tokenizer([chat_text], return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        do_sample=False,
        temperature=0.0,
        pad_token_id=tokenizer.eos_token_id
    )

    return tokenizer.decode(
        outputs[0][inputs.input_ids.shape[1]:],
        skip_special_tokens=True
    ).strip()


# ================= æ­£åˆ™ï¼šæ ¸å¿ƒè§„åˆ™ =================
# â‘  ä¸¥æ ¼ï¼šåªç”¨äºâ€œæ¡æ–‡èµ·å§‹åˆ¤å®šâ€
ARTICLE_START_PATTERN = re.compile(
    r"^\s*(ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+\s*æ¡)",
    re.MULTILINE
)


# â‘¡ å®½æ¾ï¼šåªç”¨äºâ€œæ¡å·æŠ½å–â€
ARTICLE_NUMBER_PATTERN = re.compile(
    r"(ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+\s*æ¡)"
)


def cut_first_article_from_text(text: str) -> tuple[str, str] | None:
    matches = list(ARTICLE_START_PATTERN.finditer(text))
    if not matches:
        return None

    start = matches[0].start()

    if len(matches) > 1:
        end = matches[1].start()
    else:
        end = len(text)

    return text[start:end], text[end:]


def is_valid_cut(raw_text: str) -> bool:
    s = raw_text.strip()

    # 1) å¤ªçŸ­çš„ä¸€å¾‹å½“å™ªå£°ï¼ˆæ¯”å¦‚â€œç¬¬ä¸‰åå…«æ¡è‡³â€ï¼‰
    if len(s) < 20:
        return False

    # 2) å¦‚æœæ•´æ®µå‡ ä¹å°±æ˜¯â€œç¬¬Xæ¡è‡³ç¬¬Yæ¡...â€è¿™ç§èŒƒå›´å¼•ç”¨ï¼ˆä¸”å¾ˆçŸ­ï¼‰ï¼Œæ‰è¿‡æ»¤
    #    æ³¨æ„ï¼šè¿™é‡Œåªåœ¨â€œå¼€å¤´ç´§è·Ÿè‡³â€æ‰è®¤ä¸ºæ˜¯â€œå¼•ç”¨å‹åˆ‡ç‰‡â€
    if re.match(r"^ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+\s*æ¡\s*è‡³\s*ç¬¬", s):
        return False

    # 3) å¦‚æœå¼€å¤´å°±æ˜¯â€œç¬¬Xæ¡è§„å®š...â€ä¸”æ–‡æœ¬å¾ˆçŸ­ï¼Œæ‰è®¤ä¸ºæ˜¯å¼•ç”¨ç¢ç‰‡
    #    ï¼ˆæ­£æ–‡æ¡æ–‡ä¸€èˆ¬ä¸ä¼šä»¥â€œç¬¬Xæ¡è§„å®šâ€å¼€å¤´ï¼‰
    if re.match(r"^ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+\s*æ¡\s*è§„å®š", s) and len(s) < 60:
        return False

    return True

# ================= ä¸»æµç¨‹ =================
def process_single_pdf(pdf_path: Path, model, tokenizer):
    law_name = pdf_path.stem
    print(f"\nğŸ“˜ æ­£åœ¨å¤„ç†ï¼š{law_name}")

    remaining_text = normalize_article_starts(extract_pdf_text(pdf_path))
    articles = []

    debug_dir = Path(__file__).parent / "debug_trim" / law_name
    debug_dir.mkdir(parents=True, exist_ok=True)
    step = 0

    while remaining_text.strip():
        prev_len = len(remaining_text)

        cut = cut_first_article_from_text(remaining_text)
        if not cut:
            break

        raw_text, rest = cut

        # âœ… è°ƒè¯•æ‰“å°ï¼šçœ‹çœ‹â€œè¢«å½“æˆæ¡æ–‡èµ·å§‹â€çš„åˆ°åº•åˆ‡å‡ºäº†ä»€ä¹ˆ
        print("DEBUG CUT:", raw_text[:80].replace("\n", "\\n"))

        # å¦‚æœåˆ‡å‡ºæ¥æ˜æ˜¾ä¸æ˜¯æ¡æ–‡ï¼ˆä¾‹å¦‚â€œç¬¬ä¸‰åå…«æ¡è‡³â€ï¼‰ï¼Œè·³è¿‡å®ƒï¼Œä½†å¿…é¡»è®©æ–‡æœ¬å‰è¿›
        if not is_valid_cut(raw_text):
            # è®© remaining_text å‰è¿›åˆ° restï¼Œé¿å…æ­»å¾ªç¯
            remaining_text = rest
            continue

        remaining_text = rest


        # æŠ½å–æ¡å·ï¼ˆä¸€å®šæˆåŠŸï¼‰
        m = ARTICLE_NUMBER_PATTERN.search(raw_text)
        article_number = m.group(1) if m else "æœªçŸ¥æ¡"

        cleaned = llm_clean_article(model, tokenizer, raw_text)

        articles.append({
            "law_name": law_name,
            "article_number": article_number,
            "content": cleaned
        })

        step += 1
        debug_path = debug_dir / f"step_{step:03d}.txt"
        with open(debug_path, "w", encoding="utf-8") as f:
            f.write(remaining_text)

        print(f"ğŸ“ å·²ä¿å­˜è£å‰ªåæ–‡æœ¬ï¼š{debug_path}")

        if len(remaining_text) >= prev_len:
            print("âŒ remaining_text æœªç¼©çŸ­ï¼Œç»ˆæ­¢é˜²æ­»å¾ªç¯")
            break

    # ================= è¾“å‡º =================

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    out_path = Path(OUTPUT_DIR) / f"{law_name}.json"

    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(
            {
                "law_name": law_name,
                "total_articles": len(articles),
                "articles": articles
            },
            f,
            ensure_ascii=False,
            indent=2
        )

    print(f"âœ… å®Œæˆ {law_name}ï¼Œå…±æå– {len(articles)} æ¡")


def main():
    model, tokenizer = load_model()
    for pdf_file in Path(PDF_DIR).glob("*.pdf"):
        process_single_pdf(pdf_file, model, tokenizer)


if __name__ == "__main__":
    main()
